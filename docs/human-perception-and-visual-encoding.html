<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Human Perception and Visual Encoding | Data Visualization - From a Human-Centered Perspective (Lecture Notes)</title>
  <meta name="description" content="These are the lecture notes of the course on Data Visualization at the Freie Universität Berlin." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Human Perception and Visual Encoding | Data Visualization - From a Human-Centered Perspective (Lecture Notes)" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="your book url like https://clmb.github.io/lecture_data_visualization/" />
  
  <meta property="og:description" content="These are the lecture notes of the course on Data Visualization at the Freie Universität Berlin." />
  <meta name="github-repo" content="clmb/lecture_data_visualization/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Human Perception and Visual Encoding | Data Visualization - From a Human-Centered Perspective (Lecture Notes)" />
  
  <meta name="twitter:description" content="These are the lecture notes of the course on Data Visualization at the Freie Universität Berlin." />
  

<meta name="author" content="Claudia Müller-Birn" />


<meta name="date" content="2021-11-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="understanding-your-data.html"/>
<link rel="next" href="visualizing-tabular-data.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Human-Centered DataViz</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="the-value-of-data-visualization.html"><a href="the-value-of-data-visualization.html"><i class="fa fa-check"></i><b>1</b> The Value of Data Visualization</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-value-of-data-visualization.html"><a href="the-value-of-data-visualization.html#example-visualizations"><i class="fa fa-check"></i><b>1.1</b> Example Visualizations</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="the-value-of-data-visualization.html"><a href="the-value-of-data-visualization.html#napoleons-march"><i class="fa fa-check"></i><b>1.1.1</b> Napoleon’s March</a></li>
<li class="chapter" data-level="1.1.2" data-path="the-value-of-data-visualization.html"><a href="the-value-of-data-visualization.html#cholera-epidemic-in-london"><i class="fa fa-check"></i><b>1.1.2</b> Cholera Epidemic in London</a></li>
<li class="chapter" data-level="1.1.3" data-path="the-value-of-data-visualization.html"><a href="the-value-of-data-visualization.html#space-shuttle-challenger-disaster"><i class="fa fa-check"></i><b>1.1.3</b> Space Shuttle Challenger Disaster</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="the-value-of-data-visualization.html"><a href="the-value-of-data-visualization.html#sec:hcd"><i class="fa fa-check"></i><b>1.2</b> Human-Centered Data Visualization</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-process-of-visualizing-data.html"><a href="the-process-of-visualizing-data.html"><i class="fa fa-check"></i><b>2</b> The Process of Visualizing Data</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-process-of-visualizing-data.html"><a href="the-process-of-visualizing-data.html#the-process-of-visualizing-data-1"><i class="fa fa-check"></i><b>2.1</b> The Process of Visualizing Data</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="the-process-of-visualizing-data.html"><a href="the-process-of-visualizing-data.html#the-situation-level"><i class="fa fa-check"></i><b>2.1.1</b> The situation level</a></li>
<li class="chapter" data-level="2.1.2" data-path="the-process-of-visualizing-data.html"><a href="the-process-of-visualizing-data.html#the-datatask-abstraction-level"><i class="fa fa-check"></i><b>2.1.2</b> The data/task abstraction level</a></li>
<li class="chapter" data-level="2.1.3" data-path="the-process-of-visualizing-data.html"><a href="the-process-of-visualizing-data.html#the-visual-encodinginteraction-idiom-level"><i class="fa fa-check"></i><b>2.1.3</b> The visual encoding/interaction idiom level</a></li>
<li class="chapter" data-level="2.1.4" data-path="the-process-of-visualizing-data.html"><a href="the-process-of-visualizing-data.html#the-algorithmic-level"><i class="fa fa-check"></i><b>2.1.4</b> The algorithmic level</a></li>
<li class="chapter" data-level="2.1.5" data-path="the-process-of-visualizing-data.html"><a href="the-process-of-visualizing-data.html#validity-of-your-vis-design"><i class="fa fa-check"></i><b>2.1.5</b> Validity of Your Vis Design</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="the-process-of-visualizing-data.html"><a href="the-process-of-visualizing-data.html#mapping-the-human-centered-design-process-and-the-nested-model"><i class="fa fa-check"></i><b>2.2</b> Mapping the Human-Centered Design Process and the Nested Model</a></li>
<li class="chapter" data-level="2.3" data-path="the-process-of-visualizing-data.html"><a href="the-process-of-visualizing-data.html#major-elements-of-visualization-design"><i class="fa fa-check"></i><b>2.3</b> Major Elements of Visualization Design</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-process-of-visualizing-data.html"><a href="the-process-of-visualizing-data.html#reflecting-on-data"><i class="fa fa-check"></i><b>2.3.1</b> Reflecting on Data</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-process-of-visualizing-data.html"><a href="the-process-of-visualizing-data.html#reflecting-on-tasks"><i class="fa fa-check"></i><b>2.3.2</b> Reflecting on Tasks</a></li>
<li class="chapter" data-level="2.3.3" data-path="the-process-of-visualizing-data.html"><a href="the-process-of-visualizing-data.html#the-how-of-visualization-design---at-a-glance"><i class="fa fa-check"></i><b>2.3.3</b> The How of Visualization Design - At A Glance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="understanding-your-data.html"><a href="understanding-your-data.html"><i class="fa fa-check"></i><b>3</b> Understanding your Data</a>
<ul>
<li class="chapter" data-level="3.1" data-path="understanding-your-data.html"><a href="understanding-your-data.html#understanding-the-data-context"><i class="fa fa-check"></i><b>3.1</b> Understanding the Data Context</a></li>
<li class="chapter" data-level="3.2" data-path="understanding-your-data.html"><a href="understanding-your-data.html#understanding-the-data-structure"><i class="fa fa-check"></i><b>3.2</b> Understanding the Data Structure</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="understanding-your-data.html"><a href="understanding-your-data.html#using-r-for-data-exploration"><i class="fa fa-check"></i><b>3.2.1</b> Using R for Data Exploration</a></li>
<li class="chapter" data-level="3.2.2" data-path="understanding-your-data.html"><a href="understanding-your-data.html#visualizing-data"><i class="fa fa-check"></i><b>3.2.2</b> Visualizing Data</a></li>
<li class="chapter" data-level="3.2.3" data-path="understanding-your-data.html"><a href="understanding-your-data.html#scatterplot"><i class="fa fa-check"></i><b>3.2.3</b> Scatterplot</a></li>
<li class="chapter" data-level="3.2.4" data-path="understanding-your-data.html"><a href="understanding-your-data.html#histogram"><i class="fa fa-check"></i><b>3.2.4</b> Histogram</a></li>
<li class="chapter" data-level="3.2.5" data-path="understanding-your-data.html"><a href="understanding-your-data.html#density-plot"><i class="fa fa-check"></i><b>3.2.5</b> Density Plot</a></li>
<li class="chapter" data-level="3.2.6" data-path="understanding-your-data.html"><a href="understanding-your-data.html#boxplot"><i class="fa fa-check"></i><b>3.2.6</b> Boxplot</a></li>
<li class="chapter" data-level="3.2.7" data-path="understanding-your-data.html"><a href="understanding-your-data.html#compare-distributions"><i class="fa fa-check"></i><b>3.2.7</b> Compare Distributions</a></li>
<li class="chapter" data-level="3.2.8" data-path="understanding-your-data.html"><a href="understanding-your-data.html#summary-statistics"><i class="fa fa-check"></i><b>3.2.8</b> Summary Statistics</a></li>
<li class="chapter" data-level="3.2.9" data-path="understanding-your-data.html"><a href="understanding-your-data.html#central-tendency"><i class="fa fa-check"></i><b>3.2.9</b> Central Tendency</a></li>
<li class="chapter" data-level="3.2.10" data-path="understanding-your-data.html"><a href="understanding-your-data.html#median-and-iqr"><i class="fa fa-check"></i><b>3.2.10</b> Median, and IQR</a></li>
<li class="chapter" data-level="3.2.11" data-path="understanding-your-data.html"><a href="understanding-your-data.html#outliers"><i class="fa fa-check"></i><b>3.2.11</b> Outliers</a></li>
<li class="chapter" data-level="3.2.12" data-path="understanding-your-data.html"><a href="understanding-your-data.html#skewness"><i class="fa fa-check"></i><b>3.2.12</b> Skewness</a></li>
<li class="chapter" data-level="3.2.13" data-path="understanding-your-data.html"><a href="understanding-your-data.html#covariance-and-correlation"><i class="fa fa-check"></i><b>3.2.13</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="3.2.14" data-path="understanding-your-data.html"><a href="understanding-your-data.html#correlation-matrix"><i class="fa fa-check"></i><b>3.2.14</b> Correlation Matrix</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="understanding-your-data.html"><a href="understanding-your-data.html#general-guidelines-for-eda"><i class="fa fa-check"></i><b>3.3</b> General Guidelines for EDA</a></li>
<li class="chapter" data-level="3.4" data-path="understanding-your-data.html"><a href="understanding-your-data.html#tools-and-libraries-for-data-exploration"><i class="fa fa-check"></i><b>3.4</b> Tools and Libraries for Data Exploration</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="human-perception-and-visual-encoding.html"><a href="human-perception-and-visual-encoding.html"><i class="fa fa-check"></i><b>4</b> Human Perception and Visual Encoding</a>
<ul>
<li class="chapter" data-level="4.1" data-path="human-perception-and-visual-encoding.html"><a href="human-perception-and-visual-encoding.html#human-perception"><i class="fa fa-check"></i><b>4.1</b> Human Perception</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="human-perception-and-visual-encoding.html"><a href="human-perception-and-visual-encoding.html#the-human-eye"><i class="fa fa-check"></i><b>4.1.1</b> The Human Eye</a></li>
<li class="chapter" data-level="4.1.2" data-path="human-perception-and-visual-encoding.html"><a href="human-perception-and-visual-encoding.html#color-vision"><i class="fa fa-check"></i><b>4.1.2</b> Color Vision</a></li>
<li class="chapter" data-level="4.1.3" data-path="human-perception-and-visual-encoding.html"><a href="human-perception-and-visual-encoding.html#spation-vision"><i class="fa fa-check"></i><b>4.1.3</b> Spation Vision</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="human-perception-and-visual-encoding.html"><a href="human-perception-and-visual-encoding.html#pre-attentive-processing"><i class="fa fa-check"></i><b>4.2</b> Pre-Attentive Processing</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="human-perception-and-visual-encoding.html"><a href="human-perception-and-visual-encoding.html#choice-of-encoding---bertins-guidance"><i class="fa fa-check"></i><b>4.2.1</b> Choice of Encoding - Bertin’s Guidance</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="human-perception-and-visual-encoding.html"><a href="human-perception-and-visual-encoding.html#combining-channels"><i class="fa fa-check"></i><b>4.3</b> Combining Channels</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="visualizing-tabular-data.html"><a href="visualizing-tabular-data.html"><i class="fa fa-check"></i><b>5</b> Visualizing Tabular Data</a>
<ul>
<li class="chapter" data-level="5.1" data-path="visualizing-tabular-data.html"><a href="visualizing-tabular-data.html#visual-encoding-for-numerical-data"><i class="fa fa-check"></i><b>5.1</b> Visual Encoding for Numerical Data</a></li>
<li class="chapter" data-level="5.2" data-path="visualizing-tabular-data.html"><a href="visualizing-tabular-data.html#Visual-Encoding-for-Categorical-Data"><i class="fa fa-check"></i><b>5.2</b> Visual Encoding for Categorical Data</a></li>
<li class="chapter" data-level="5.3" data-path="visualizing-tabular-data.html"><a href="visualizing-tabular-data.html#spatial-axis-orientation"><i class="fa fa-check"></i><b>5.3</b> Spatial Axis Orientation</a></li>
<li class="chapter" data-level="5.4" data-path="visualizing-tabular-data.html"><a href="visualizing-tabular-data.html#spatial-layout-density"><i class="fa fa-check"></i><b>5.4</b> Spatial Layout Density</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Visualization - From a Human-Centered Perspective (Lecture Notes)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="human-perception-and-visual-encoding" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Human Perception and Visual Encoding</h1>
<p>As we have discussed on the previous chapters, an important first step in data visualization is to contextualize your data. The next step is to select - based on the data characteristics - an effective visual encoding. Such visual encoding maps the data values to graphical features such as position, size, shape, and color. However, such mapping is not as straightforward as one might assume. A prerequisite is to understand how we, as humans, perceive our world, or more specifically visualizations. Ware states in his book <span class="citation">(<a href="#ref-ware2019information" role="doc-biblioref">Ware 2019</a>)</span>: ‘’Understanding human perception can significantly improve both the quality and the quantity of information being displayed.’’ Such understanding allows us to design visualizations that can replace demanding cognitive calculations with simple perceptual inferences which often improve interpretability &amp; comprehension and, therefore, decision making <span class="citation">(<a href="#ref-Heer_Bostock_Ogievetsky_2010" role="doc-biblioref">Heer, Bostock, and Ogievetsky 2010</a>)</span>.</p>
<div id="human-perception" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Human Perception</h2>
<p>Human perception is the ability to perceive our surroundings through the light that enters the eyes. The eye convert light into a series of electrochemical signals that are transmitted to the brain. This process can take as little as 13 milliseconds, according to a 2017 study by MIT in the United States <span class="citation">(<a href="#ref-munzner2014visualization" role="doc-biblioref">Munzner 2014</a>)</span>. However, human vision has a number of physical and perceptual limitations (concerning colors, patterns, and structures), which we should be aware of in order to create more effective data visualizations.</p>
<p>We can roughly divide perception into two stages <span class="citation">(<a href="#ref-Few2004showmenumbers" role="doc-biblioref">Few 2004</a>)</span>. The sensations is the physical reception of the stimulus from the outside world, and the perception is a cognitive process that relates to the processing and interpretation of that stimulus. On the one hand the physical properties of the eye and the visual system mean that there are certain things that cannot be seen by the human; on the other hand, the interpretative capabilities of visual processing allow images to be constructed from incomplete information.</p>
<p>We need to understand both stages as both influence what can and cannot be perceived visually by a human being, which in turn directly affects the way that we design visualizations.</p>
<p>We begin with looking at the eye as a physical receptor, and then go on to consider the processing involved in basic vision.</p>
<div id="the-human-eye" class="section level3" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> The Human Eye</h3>
<p>Vision begins with light which is reflected from objects in the world. This light catches the eye and the image of these objects is projected upside down on the back of the eye, on the retina (this you might remember from school). In the following, we look into these components in more detail (cp. Figure XX).</p>
<!--https://upload.wikimedia.org/wikipedia/commons/f/f5/Human_eye_diagram-sagittal_view-NEI.jpg -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-8"></span>
<img src="images/humaneye.jpg" alt="Components of the Human Eye, National Eye Institute. Taken from Wikipedia Commons" width="50%" />
<p class="caption">
FIGURE4.1: Components of the Human Eye, National Eye Institute. Taken from Wikipedia Commons
</p>
</div>
<p>At the front of the eye are the cornea and lens which focus the light into a sharp image on the retina. The retina is the the light-sensitive layer of the eye. In a person with normal vision, the lens focuses images perfectly on a small depression in the back of the eye called the fovea (in German Sehgrube), which is part of the retina.</p>
<p>The retina contains two types of photoreceptor (light-sensitive cells): rods for low-light vision and cones for color vision.</p>
<p>Rods are highly sensitive to light and therefore allow us to see under a low level of lightning. However, they are unable to resolve fine detail and are subject to light saturation. This is the reason for the temporary blindness we get when moving from a darkened room into sunlight: the rods have been active and are saturated by the sudden light. As you can imagine, nowadays in our industrialized world, rods are hardly used <span class="citation">(<a href="#ref-Johnson2014designingwiththemind" role="doc-biblioref">Johnson 2014</a>)</span>. There are approximately 120 million rods per eye which are mainly situated towards the edges of the retina. Rods therefore dominate peripheral vision.</p>
<p>The cones are specialized types of photoreceptors that work best in bright light conditions. Cones are very sensitive to acute detail and provide tremendous spatial resolution. They are also directly involved in our ability to perceive color. The eye has approximately 6 million cones, mainly concentrated on the fovea. We can differentiate three types of cones. Each type of cone is sensitive to a range of light frequencies, and these sensitivity ranges overlap considerably <span class="citation">(<a href="#ref-Johnson2014designingwiththemind" role="doc-biblioref">Johnson 2014</a>)</span>.</p>
<!-- - Low frequency: these cones are sensitive to light over almost the entire range of visible light, but are most sensitive to the middle (yellow) and low (red) frequencies.
- Medium frequency: these cones respond to light ranging from the high-frequency blues through the lower middle-frequency yellows and oranges. Over- all, they are less sensitive than the low-frequency cones.
- High frequency: these cones are most sensitive to light at the upper end of the visible light spectrum—violets and blues—but they also respond weakly to middle frequencies, such as green. These cones are much less sensitive overall than the other two types of cones, and also less numerous. One result is that our eyes are much less sensitive to blues and violets than to other colors. 

Given the odd relationships among the sensitivities of our three types of retinal cone cells, one might wonder how the brain combines the signals from the cones to allow us to see a broad range of colors.
The answer is by subtraction. Neurons in the visual cortex at the back of our brain subtract the signals coming over the optic nerves from the medium- and low- frequency cones, producing a red–green difference signal channel. Other neurons in the visual cortex subtract the signals from the high- and low-frequency cones, yielding a yellow–blue difference signal channel. A third group of neurons in the visual cortex adds the signals coming from the low- and medium-frequency cones to produce an overall luminance (or black–white) signal channel.2 These three channels are called color-opponent channels.
The brain then applies additional subtractive processes to all three color-opponent channels: signals coming from a given area of the retina are effectively subtracted from similar signals coming from nearby areas of the retina.-->
<p>Although the retina is mainly covered with photoreceptors there is one blind spot where the optic nerve enters the eye. The blind spot has no rods or cones, yet our visual system compensates for this so that in normal circumstances we are unaware of it.</p>
<p>The retina also has specialized nerve cells called ganglion cells. There are two types: X-cells, which are concentrated in the fovea and are responsible for the early detection of pattern; and Y-cells which are more widely distributed in the retina and are responsible for the early detection of movement. The distribution of these cells means that, while we may not be able to detect changes in pattern in peripheral vision, we can perceive movement.</p>
</div>
<div id="color-vision" class="section level3" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Color Vision</h3>
<p>We can describe color by different color space presentation. The RGB is the most popular one. However, in 1970, computer graphics researchers developed a model that closely aligns with the way human vision perceives color: the HSL (for hue, saturation, lightness) model. Hue is described in units of degree starting from 0 (red) to 360º.These are the typical colors you know. Saturation measures the degree to which a particular hue fully exhibits its essence. It goes from 0 % (grey) to 100% (very colorful). Lightness (or brightness) measures the degree to which color appears dark or light ranging from 0% (black) to fully 100% (white). To convert your colors, you can use webpages such as <a href="https://convertacolor.com/">ConvertColor</a> or libraries such as ‘colorspace’ in GNU R <span class="citation">(<a href="#ref-Zeileisetal2019colorspace" role="doc-biblioref">Zeileis et al. 2019</a>)</span>. All HCL-based color palettes are also provided as discrete, continuous, and binned color scales for the use with the ggplot2 package [<span class="citation"><a href="#ref-Wickham2016ggplot2" role="doc-biblioref">Wickham</a> (<a href="#ref-Wickham2016ggplot2" role="doc-biblioref">2016</a>)</span>).</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="human-perception-and-visual-encoding.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;colorspace&quot;</span>)</span>
<span id="cb48-2"><a href="human-perception-and-visual-encoding.html#cb48-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-3"><a href="human-perception-and-visual-encoding.html#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="fu">hcl_palettes</span>(<span class="at">plot =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="Human-Centered-Data-Viz_files/figure-html/chunk-label4-1-1.png" width="672" /></p>
<p>However, our color vision is limited <span class="citation">(<a href="#ref-Johnson2014designingwiththemind" role="doc-biblioref">Johnson 2014</a>)</span>. Our visual system is much more sensitive to differences in color and brightness, i.e., to contrasting colors and edges, than to absolute brightness levels. We perceive colors not in absolute terms (see three types of cones) but as the difference between the color we are focusing on and the color that surrounds it <span class="citation">(<a href="#ref-Few2009nowtoseeit" role="doc-biblioref">Few 2009</a>)</span>. Our vision is therefore heavily impacted by the context.</p>
<p>There are three factors that affect our ability to distinguish colors from each other <span class="citation">(<a href="#ref-Johnson2014designingwiththemind" role="doc-biblioref">Johnson 2014</a>)</span>. First, the paler (less saturated) two colors are, the harder it is to tell them apart. Second, the the smaller or thinner objects are, the harder it is to distinguish their colors. Text is often thin, so the exact color of text is often hard to determine. Finally, the more separated color patches are, the more difficult it is to distinguish their colors, especially if the separation is great enough to require eye motion between patches.</p>
<p>But there are also external factors that influence how we can distinguish colors. Those factors are among others: the variation among color displays, gray-scale displays, display angle, and ambient lighting. These external factors are usually beyond your control but you should keep them in mind. Colors that appear distinguishable on your screen may not be so distinguishable in some of the environments in which the software is used.</p>
<!-- https://thenode.biologists.com/data-visualization-with-flying-colors/research/ -->
<p>A last issue you should consider is color blindness. Actual this term is mislabeled. It is not blindness, but rather a lack of color vision. It is the inability (or sometimes diminished ability) to see certain colors or perceive color contrasts in normal light. Color blindness is usually genetic, but in some cases it can be caused by disease or age. Furthermore, one in 12 men is color blind, compared to one in 200 women. The most common form of color blindness is red/green color blindness (protanopia). There are other, less common forms of color blindness that also involve different color pairs but also rare forms where colors can not be distinguished at all (achromatopsia).</p>
<p>To avoid that people cannot interpret your data visualizations correctly, you should use services such as <a href="https://colorbrewer2.org">ColorBrewer</a>. It supports you to select an appropriate color scheme by considering various user-selected criteria, including colorblind-friendliness.</p>
</div>
<div id="spation-vision" class="section level3" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Spation Vision</h3>
<p>The spatial resolution of the human visual field drops greatly from the center to the edges. In the center 1% of your visual field, i.e., the fovea, you have a high-resolution TIFF, and everywhere else, you have only a low-resolution JPEG. There are three reasons for this <span class="citation">(<a href="#ref-Johnson2014designingwiththemind" role="doc-biblioref">Johnson 2014</a>)</span>:</p>
<ul>
<li>Pixel density. Each eye has 6 to 7 million retinal cone cells. They are packed much more tightly in the fovea. The fovea has about 158,000 cone cells in each square millimeter. The rest of the retina has only 9,000 cone cells per square millimeter.</li>
<li>Data compression. Cone cells in the fovea connect 1:1 to the ganglial neuron cells that begin the processing and transmission of visual data, while elsewhere on the retina, multiple photoreceptor cells (cones and rods) connect to each ganglion cell. In technical terms, information from the visual periphery is compressed (with data loss) before transmission to the brain, while information from the fovea is not.</li>
<li>Processing resources. The fovea is only about 1% of the retina, but the brain’s visual cortex devotes about 50% of its area to input from the fovea. The other half of the visual cortex processes data from the remaining 99% of the retina.</li>
</ul>
<p>The result is that our vision has much, much greater resolution in the center of our visual field than elsewhere. If our peripheral vision has such low resolution, why do we see our surroundings sharply and clearly?</p>
<p>We experience this illusion because our eyes move rapidly and constantly about three times per second even when we don’t realize it, focusing our fovea on selected pieces of our environment. Our brain fills in the rest in a gross, impressionistic way based on what we know and expect. Our brain does not have to maintain a high-resolution mental model of our environment because it can order the eyes to sample and resample details in the environment as needed. We need this insights when we talk in the Chapter Interaction (Section <a href="#sec:interaction"><strong>??</strong></a>).</p>
<!-- https://www.interaction-design.org/literature/article/preattentive-visual-properties-and-how-to-use-them-in-information-visualization -->
</div>
</div>
<div id="pre-attentive-processing" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Pre-Attentive Processing</h2>
<p>Many visual channels provide pre-attentive processing, where a distinct item stands out from many others immediately. The great value of preattentive processing is that the time it takes us to detect the different object does not depend on the number of distractor objects. Our low-level visual system performs massive parallel processing on these visual channels without requiring the viewer to consciously pay direct attention to the individual elements. Pre-attentive processing occurs for many channels. Examples are tilt, size, shape, proximity, and even shadow direction, but also various types of motion such as flicker, motion direction, and motion speed. However, a small number of potential channels do not support pre-attentive processing. One example is parallelism. Most visual channel pairs do not support pre-attentive processing, but some pairs do: one example is space and hue, and another is motion and shape. Pre-attentive processing is definitely not possible with three or more channels.</p>
<p>A question is, how we can systematically approach pre-attentive processing. Two principles can guide the use of visual channels in visual encoding: expressiveness and effectiveness. A set of facts that is <em>expressible</em> in a visual language if the sentences (i.e. the visualizations) in the language express all the facts in the set of data, and only the facts in the data. A visualization is more <em>effective</em> than another visualization if the information conveyed by one visualization is more readily perceived than the information in the other visualization.</p>
<p>The expressiveness principle dictates that the visual encoding should express all of, and only, the information that the dataset exhibits. For example, ordered data (nominal) should be shown in a way that our perceptual system intrinsically senses as ordered and unordered data (ordinal) should not be shown in a way that perceptually implies an ordering. The expressiveness principle is especially important, wenn we talk about ethics in data visualization (Section <a href="#sec:ethics"><strong>??</strong></a>)</p>
<p>The effectiveness principle dictates that the importance of the attribute should match the salience of the channel, i.e. its noticeability. We already talked about colors but there are more visual channels we can consider when we decide about the visual encoding. The first researcher who thought about the effectiveness of visualizations is <a href="https://en.wikipedia.org/wiki/Jacques_Bertin">Jacques Bertin</a>.</p>
<!-- https://graphworkflow.com/retinal/ -->
<!-- Check out: A Survey of Perception-Based Visualization Studies by Task -->
<div id="choice-of-encoding---bertins-guidance" class="section level3" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Choice of Encoding - Bertin’s Guidance</h3>
<p>In 1967, Jacques Bertin published the book “Semiologie Graphique” (in English “Semiology of Graphics”) that was based on his long-standing experience as a cartographer and geographer. In this book, Bertin linked human perception to visualization. Even though, this linking was more based on intuition than vision research, Bertin’s experiences were later empirically proven. Bertin’s key concept is the image, which is is the fundamental perceptual unit of a visualization. An ideal visualizations will contain only a single image in order to optimize “efficiency,” the speed with which observer can extract the information.</p>
<p>Bertin identified in his work that every visualization is made by a series of basic components that have different expressive power and that each one works best only in some conditions. In general, the encoding of data can be done in a coordinate system with the cartesian coordinate system as its most prominent representative. Of course, there are further coordinate systems, such as geographical coordinate system, parallel coordinates system, polar coordinate system, or the network coordinate system. Based on the chosen coordinate system, you need to place your data, more precisely your data values (e.g., items, links) into these coordinates. For this, you can differentiate so-called <em>marks</em>. A mark is a basic graphical element which can be classified according to the number of spatial dimensions they require <span class="citation">(<a href="#ref-munzner2014visualization" role="doc-biblioref">Munzner 2014</a>)</span>: Possible dimensions are: a zero-dimensional (0D) mark is a point, a one-dimensional (1D) mark is a line, a two-dimensional (2D) mark is an area, a three- dimensional (3D) mark defines a volume.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-9"></span>
<img src="images/bertin_visualattributes.png" alt="Bertin’s Visual Channels Taken from McDonald (1999).)" width="50%" />
<p class="caption">
FIGURE4.2: Bertin’s Visual Channels Taken from McDonald (1999).)
</p>
</div>
<p>A visual channel defines the appearance of <em>marks</em> (points, lines, areas), independent its dimensionality. Bertin differentiated six visual channels: size, value, texture, color, orientation, shape. For each of these channels he pointed out in what cases they work best and how to use them (cp. Figure XX, redrawn from <span class="citation">(<a href="#ref-MacDonald1999usingcolor" role="doc-biblioref">MacDonald 1999</a>)</span>). We already discussed that the human eye is independently sensitive to these visual channels, which means that more than one visual channel can be deployed at the same time in order to encode different variation in the data.</p>
<!-- https://graphworkflow.com/retinal/ -->
<p>These visual channels can represent different relationships between marks:</p>
<ul>
<li>association (<span class="math inline">\(\equiv\)</span>): the marks an be perceived as similar (<em>group</em>),</li>
<li>selection (<span class="math inline">\(\ncong\)</span>): the marks are perceived as different, forming families (<em>distinguish</em>),</li>
<li>order (<span class="math inline">\(O\)</span>): the marks are perceived as ordered (<em>sort</em>), and</li>
<li>quantity (<span class="math inline">\(Q\)</span>): the marks are perceived as proportional to each other (<em>count</em>).</li>
</ul>
<p>These perceptional properties can be arranged into the levels of organization <span class="citation">(<a href="#ref-green1998toward" role="doc-biblioref">Green 1998</a>)</span>. Associative perception is the lowest level of organization. It allows grouping all elements of a variable in spite of different values. ‘’Selective perception’’ is the next higher level (flip side of association). It permits the viewer to select one category of a component, perceive locations of objects in that category and ignore others. Order allows the data to be ordinally ranked. An observer can see that one value of a variable represents a larger or smaller quantity than another. Quantity permits direct extraction of ratios, without need of consulting a legend, etc.</p>
<!-- Take examples from here: https://github.com/rseiter/ClevelandDataVis -->
<p>As said, Bertin’s research has been empirically substantiated. Most notable are perhaps Cleveland and McGill’s controlled experiments <span class="citation">(<a href="#ref-ClevelandMcGill1984graphicalperception" role="doc-biblioref">Cleveland and McGill 1984</a>)</span>. The most important findings is that they mapped human response directly to visually encoded abstract information and provide explicit rankings of perceptual accuracy for each channel type.</p>
<!-- Cleveland and McGill's experiments on magnitude channels showed that aligned position against a common scale is perceived most accurately, followed by unaligned position against an identical scale, followed by length, followed by angle. Area rankings are much less accurate than all others. They also suggest rankings for channels they have not directly tested: After area is an equivalence class of volume, curvature, and luminance; this class is followed by a hue in first place. (This last-place ranking is for hue as a magnitude channel, a very different matter from its second-place ranking as an identity channel). Cleveland and McGill's use their insights for redesigning existing graphs. -->
<p>Based on this work, Mackinlay <span class="citation">(<a href="#ref-Mackinlay1986automatingdesign" role="doc-biblioref">Mackinlay 1986</a>)</span> has derived perceptually-motivated rankings of the effectiveness of variables such as position, length, area, and color for encoding quantitative data. Heer and Bostock <span class="citation">(<a href="#ref-HeerBostock2010crowdsourcing" role="doc-biblioref">Heer and Bostock 2010</a>)</span> confirmed and extended this work through crowdsourcing. The only discrepancy is that the later research found length and angle judgments that are roughly equivalent.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-10"></span>
<img src="images/stevens_psychophysicalpowerlaw.png" alt="Results of Psychophysical power law of Stevens. Taken from Munzner (2014).)" width="50%" />
<p class="caption">
FIGURE4.3: Results of Psychophysical power law of Stevens. Taken from Munzner (2014).)
</p>
</div>
<p>Their results for visual encodings agree well with psychophysical channel measurements. Psychophysics is devoted to the systematic measurement of general human perception <span class="citation">(<a href="#ref-StevensMarks2017Psychophysics" role="doc-biblioref">Stevens and Marks 2017</a>)</span>. We perceive different visual channels with different degrees of accuracy; they are not all equally distinguishable. Our responses to the sensory experience of size can be characterized by power-law/power-law, where the exponent depends on the precise sensory modality: Most stimuli are magnified or compressed; few remain unchanged. The diagram in Figure XX shows that length has an exponent of n = 1.0, so our perception of length is very close to the true value. Length means the length of a line segment on a 2D plane perpendicular to the observer. The other visual channels are not perceived as accurately: Area and brightness are compressed, while color saturation or electroshocks are magnified.</p>
</div>
</div>
<div id="combining-channels" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Combining Channels</h2>
<p>Multiple visual channels can be combined to redundantly encode the same attribute. The limitation of this approach is that more channels are ‘’consumed’’ so not as many attributes can be encoded in total, but the advantage is that the attributes represented are very well perceived.</p>
<p>However, visual channels are not completely independent of each other, since some have dependencies and interactions with others <span class="citation">(<a href="#ref-munzner2014visualization" role="doc-biblioref">Munzner 2014</a>)</span>. You must consider a continuum of potential interactions between channels for each pair, ranging from orthogonal and independent separable channels to inseparably combined integrated channels. Visual encoding is straightforward for single channels, but attempts to encode different information in integrated channels will fail. People will not be able to access the desired information about each attribute, but an unexpected combination will be perceived.</p>
<p><em>Separability of Visual Channels</em>: A pair of channels that are completely separable is position and hue.
An example of interference between the channels, showing that size is not completely separable from hue. Size interacts with many visual channels, including shape.
Integral pair is an encoding of one variable with horizontal size and another with vertical size. It is ineffective because what we perceive directly is the planar size of the circles, namely their area.
An inseparable pair of channels is the red and green channels of the RGB color space. These channels are not perceived separately, but are integrated into a combined color perception, so the three channels are not perceptual.</p>
<p><em>Grouping of Visual Channels</em>:
The encoding of link markers using containment/containment or link lines conveys the information that the linked objects form a group with a very strong perceptual cue. Containment is the strongest cue for grouping, with linkage a close second.
Another way to convey that elements form a group is to encode categorical data according to identity channels. All elements that share the same level of categorical attribute can be perceived as a group simply by selectively directing attention to that level. The perceptual grouping cue of identity channels is not as strong as using link or containment markers, but one advantage of this lightweight approach is that it does not add additional clutter in the form of extra link markers.
The third strongest grouping approach is proximity, which is the placement of objects within the same spatial region. This phenomenon of perceptual grouping is the reason why the best placed channel for encoding categorical data is a spatial region.
The final grouping channel is the similarity to the other categorical channels of hue and motion, and also shape if carefully selected. Logically, proximity is like similarity for spatial position; however, from a perceptual perspective, the effect of the spatial channels is so much stronger than the effect of the others that it makes sense to consider them separately.</p>
<!-- https://flylib.com/books/en/2.412.1/gestalt_principles_of_visual_perception.html -->

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-ClevelandMcGill1984graphicalperception" class="csl-entry">
Cleveland, William S., and Robert McGill. 1984. <span>“Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods.”</span> <em>Journal of the American Statistical Association</em> 79 (387): 531–54.
</div>
<div id="ref-Few2004showmenumbers" class="csl-entry">
Few, Stephen. 2004. <em>Show Me the Numbers: Designing Tables and Graphs to Enlighten</em>. Analytics Press.
</div>
<div id="ref-Few2009nowtoseeit" class="csl-entry">
———. 2009. <em>Now You See It: An Introduction to Visual Data Sensemaking</em>. First edition. Analytics Press.
</div>
<div id="ref-green1998toward" class="csl-entry">
Green, Marc. 1998. <span>“Toward a Perceptual Science of Multidimensional Data Visualization: Bertin and Beyond.”</span> <em>ERGO/GERO Human Factors Science</em> 8: 1–30.
</div>
<div id="ref-HeerBostock2010crowdsourcing" class="csl-entry">
Heer, Jeffrey, and Michael Bostock. 2010. <span>“Crowdsourcing Graphical Perception: Using Mechanical Turk to Assess Visualization Design.”</span> In <em>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em>, 203–12. CHI ’10. Association for Computing Machinery. <a href="https://doi.org/10.1145/1753326.1753357">https://doi.org/10.1145/1753326.1753357</a>.
</div>
<div id="ref-Heer_Bostock_Ogievetsky_2010" class="csl-entry">
Heer, Jeffrey, Michael Bostock, and Vadim Ogievetsky. 2010. <span>“A Tour Through the Visualization Zoo.”</span> <em>Communications of the ACM</em> 53 (6): 59–67. <a href="https://doi.org/10.1145/1743546.1743567">https://doi.org/10.1145/1743546.1743567</a>.
</div>
<div id="ref-Johnson2014designingwiththemind" class="csl-entry">
Johnson, Jeff. 2014. <em>Designing with the Mind in Mind: Simple Guide to Understanding User Interface Design Guidelines</em>. Second edition. Elsevier, Morgan Kaufmann is an imprint of Elsevier.
</div>
<div id="ref-MacDonald1999usingcolor" class="csl-entry">
MacDonald, L. W. 1999. <span>“Using Color Effectively in Computer Graphics.”</span> <em>IEEE Computer Graphics and Applications</em> 19 (4): 20–35. <a href="https://doi.org/10.1109/38.773961">https://doi.org/10.1109/38.773961</a>.
</div>
<div id="ref-Mackinlay1986automatingdesign" class="csl-entry">
Mackinlay, Jock. 1986. <span>“Automating the Design of Graphical Presentations of Relational Information.”</span> <em>ACM Transactions on Graphics</em> 5 (2): 110–41. <a href="https://doi.org/10.1145/22949.22950">https://doi.org/10.1145/22949.22950</a>.
</div>
<div id="ref-munzner2014visualization" class="csl-entry">
Munzner, Tamara. 2014. <em>Visualization Analysis and Design</em>. CRC press.
</div>
<div id="ref-StevensMarks2017Psychophysics" class="csl-entry">
Stevens, Stanley Smith, and Lawrence E. Marks. 2017. <em>Psychophysics: Introduction to Its Perceptual, Neural, and Social Prospects</em>. Routledge.
</div>
<div id="ref-ware2019information" class="csl-entry">
Ware, Colin. 2019. <em>Information Visualization: Perception for Design</em>. Morgan Kaufmann.
</div>
<div id="ref-Wickham2016ggplot2" class="csl-entry">
Wickham, Hadley. 2016. <em>Ggplot2: Elegant Graphics for Data Analysis</em>. Springer-Verlag New York. <a href="https://ggplot2.tidyverse.org">https://ggplot2.tidyverse.org</a>.
</div>
<div id="ref-Zeileisetal2019colorspace" class="csl-entry">
Zeileis, Achim, Jason C. Fisher, Kurt Hornik, Ross Ihaka, Claire D. McWhite, Paul Murrell, Reto Stauffer, and Claus O. Wilke. 2019. <span>“Colorspace: A Toolbox for Manipulating and Assessing Colors and Palettes.”</span> <em>arXiv:1903.06490 [Cs, Stat]</em>, March. <a href="http://arxiv.org/abs/1903.06490">http://arxiv.org/abs/1903.06490</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="understanding-your-data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="visualizing-tabular-data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
